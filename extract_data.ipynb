{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196f7e1c-ea72-4586-9648-194a48698c3f",
   "metadata": {},
   "source": [
    "## DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f556a5-6677-4f09-9504-86a5a578628b",
   "metadata": {},
   "source": [
    "### Extraction sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db5358-5e65-4a22-806d-f738026e7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local files â†’ COSING_Annex_III_v2.xls, product_info.csv, cosmetics.csv\n",
    "# REST API â†’ Open Beauty Facts API (cosmetic products, JSON format)\n",
    "# Web scraping â†’ Wikipedia HTML page (HTML parsing with BeautifulSoup)\n",
    "# MySQL database â†’ sephora_products / skincare_products tables already imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876ae045-befa-473a-99f7-f2302587eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a9dd6c-247e-4e9c-9524-6ea36c209955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db668840-0453-420a-87db-abb23592051a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables (.env) \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2079ee-3272-4bf6-9984-ba2d15f8a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration \n",
    "BASE_DIR      = Path.cwd()\n",
    "RAW_DIR       = BASE_DIR / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f508208-8961-4260-8878-2bdfbee266d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "LOG_FILE = BASE_DIR / \"logs\" / \"extraction.log\"\n",
    "LOG_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE, encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d1752fc-0205-4b31-9a68-12390123220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parameters from .env\n",
    "MYSQL_USER     = os.getenv(\"MYSQL_USER\",     \"root\")\n",
    "MYSQL_PASSWORD = os.getenv(\"MYSQL_PASSWORD\", \"\")\n",
    "MYSQL_HOST     = os.getenv(\"MYSQL_HOST\",     \"localhost\")\n",
    "MYSQL_PORT     = os.getenv(\"MYSQL_PORT\",     \"3306\")\n",
    "MYSQL_DB       = os.getenv(\"MYSQL_DB\",       \"cosmetics_regulatory_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea6113a5-d558-429d-b002-f7b860b43636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file paths\n",
    "FILE_COSING   = os.getenv(\"FILE_COSING\",   str(BASE_DIR / \"data\" / \"raw\" / \"COSING_Annex_III_v2.xls\"))\n",
    "FILE_SEPHORA  = os.getenv(\"FILE_SEPHORA\",  str(BASE_DIR / \"data\" / \"raw\" / \"product_info.csv\"))\n",
    "FILE_SKINCARE = os.getenv(\"FILE_SKINCARE\", str(BASE_DIR / \"data\" / \"raw\" / \"cosmetics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c9b8e63-219e-42b6-94d8-0d94eef2d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the REST API and web scraping\n",
    "API_OPEN_BEAUTY_FACTS_URL      = \"https://world.openfoodfacts.org/cgi/search.pl\"\n",
    "SCRAPING_WIKIPEDIA_COSM_URL    = \"https://en.wikipedia.org/wiki/Ingredients_of_cosmetics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11eacf-426b-436d-972c-558051f33c2c",
   "metadata": {},
   "source": [
    "### 1. EXTRACTION FROM LOCAL FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95d9b5f-cee5-4fd3-84f3-60a01ca3e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_files() -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads the three local raw data files.\n",
    "    Returns a dictionary {source_name: DataFrame}.\n",
    "    \"\"\"\n",
    "    logger.info(\"â”€â”€ Source 1: Extraction from local files â”€â”€\")\n",
    "    results = {}\n",
    "\n",
    "    # 1a. COSING (Excel file)\n",
    "    try:\n",
    "        logger.info(f\"  Reading {FILE_COSING} ...\")\n",
    "        df = pd.read_excel(FILE_COSING, engine=\"xlrd\")\n",
    "        results[\"cosing\"] = df\n",
    "        logger.info(f\"  âœ“ COSING loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"  âœ— File not found: {FILE_COSING}\")\n",
    "        results[\"cosing\"] = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"  âœ— Error reading COSING: {e}\")\n",
    "        results[\"cosing\"] = pd.DataFrame()\n",
    "\n",
    "    # 1b. SEPHORA (CSV file)\n",
    "    try:\n",
    "        logger.info(f\"  Reading {FILE_SEPHORA} ...\")\n",
    "        df = pd.read_csv(FILE_SEPHORA)\n",
    "        results[\"sephora\"] = df\n",
    "        logger.info(f\"  âœ“ SEPHORA loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"  âœ— File not found: {FILE_SEPHORA}\")\n",
    "        results[\"sephora\"] = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"  âœ— Error reading SEPHORA: {e}\")\n",
    "        results[\"sephora\"] = pd.DataFrame()\n",
    "\n",
    "    # 1c. SKINCARE (CSV file)\n",
    "    try:\n",
    "        logger.info(f\"  Reading {FILE_SKINCARE} ...\")\n",
    "        df = pd.read_csv(FILE_SKINCARE)\n",
    "        results[\"skincare\"] = df\n",
    "        logger.info(f\"  âœ“ SKINCARE loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"  âœ— File not found: {FILE_SKINCARE}\")\n",
    "        results[\"skincare\"] = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"  âœ— Error reading SKINCARE: {e}\")\n",
    "        results[\"skincare\"] = pd.DataFrame()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363591c2-1e22-41a8-a60c-60f8c85fb65e",
   "metadata": {},
   "source": [
    "### 2. EXTRACTION VIA REST API (Open Beauty Facts â€” JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f93d2ce-55ee-473e-aaa5-b072499d14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_api(max_pages: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calls the Open Beauty Facts API with json=1 â†’ retrieves JSON directly.\n",
    "    Parameter:\n",
    "        max_pages: number of result pages to fetch.\n",
    "    Returns a DataFrame with the extracted products.\n",
    "    \"\"\"\n",
    "    logger.info(\"â”€â”€ Source 2: Extraction via REST API (Open Beauty Facts â€” JSON) â”€â”€\")\n",
    "\n",
    "    all_products = []\n",
    "    headers = {\"User-Agent\": \"Cosmetics-Regulatory-Project/1.0 (educational)\"}\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        try:\n",
    "            logger.info(f\"  API request page {page}/{max_pages} ...\")\n",
    "            params = {\n",
    "                \"search_terms\": \"cosmetics\",\n",
    "                \"search_simple\": \"1\",\n",
    "                \"action\": \"process\",\n",
    "                \"json\": \"1\",            # â† request JSON\n",
    "                \"page\": page,\n",
    "                \"count\": 25\n",
    "            }\n",
    "            response = requests.get(\n",
    "                API_OPEN_BEAUTY_FACTS_URL,\n",
    "                params=params,\n",
    "                headers=headers,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            products = data.get(\"products\", [])\n",
    "\n",
    "            if not products:\n",
    "                logger.info(f\"  No products on page {page}, stopping pagination.\")\n",
    "                break\n",
    "\n",
    "            for product in products:\n",
    "                all_products.append({\n",
    "                    \"product_name\":     product.get(\"product_name\", \"\"),\n",
    "                    \"brand\":            product.get(\"brands\", \"\"),\n",
    "                    \"ingredients_text\": product.get(\"ingredients_text\", \"\"),\n",
    "                    \"categories\":       product.get(\"categories\", \"\"),\n",
    "                    \"country\":          product.get(\"countries\", \"\"),\n",
    "                    \"barcode\":          product.get(\"code\", \"\"),\n",
    "                    \"source\":           \"open_beauty_facts_api\"\n",
    "                })\n",
    "\n",
    "            logger.info(f\"  âœ“ Page {page}: {len(products)} products retrieved\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.warning(f\"  âš  Timeout on page {page} â€” continuing with next page.\")\n",
    "            continue\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            logger.error(f\"  âœ— HTTP error page {page}: {e}\")\n",
    "            continue\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"  âœ— Network error page {page}: {e}\")\n",
    "            break\n",
    "        except (ValueError, KeyError) as e:\n",
    "            logger.error(f\"  âœ— JSON parsing error page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(all_products)\n",
    "    logger.info(f\"  âœ“ REST API â€” total: {len(df)} products extracted\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd37e2dd-514f-431e-92e6-b3584f4a651c",
   "metadata": {},
   "source": [
    "### 3. EXTRACTION VIA WEB SCRAPING (Wikipedia â€” Ingredients of cosmetics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3643935-a462-4d7c-a887-14613262e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_by_scraping() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape the Wikipedia page \"Ingredients of cosmetics\" to extract\n",
    "    the list of common cosmetic ingredients and their links.\n",
    "\n",
    "    Confirmed DOM structure:\n",
    "        - The heading is <h2 id=\"Common_ingredients\"> inside a\n",
    "          <div class=\"mw-heading mw-heading2\">\n",
    "        - The grandparent of this <div> contains all <p> elements of the section\n",
    "        - Each <p> contains <a href=\"/wiki/...\"> links to ingredients\n",
    "        - Stop when the next <div class=\"mw-heading mw-heading2\"> is encountered\n",
    "          (which is \"Types of cosmetics\")\n",
    "\n",
    "    Returns a DataFrame with: ingredient, wikipedia_link, description, source\n",
    "    \"\"\"\n",
    "    logger.info(\"â”€â”€ Source 3: Extraction via Web Scraping (Wikipedia â€” Ingredients of cosmetics) â”€â”€\")\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    scraped_ingredients = []\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"  GET request to {SCRAPING_WIKIPEDIA_COSM_URL} ...\")\n",
    "        response = requests.get(SCRAPING_WIKIPEDIA_COSM_URL, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        logger.info(f\"  âœ“ HTML page retrieved â€” size: {len(response.text)} characters\")\n",
    "\n",
    "        # â”€â”€ Find the \"Common_ingredients\" heading â”€â”€\n",
    "        heading_h2 = soup.find(\"h2\", {\"id\": \"Common_ingredients\"})\n",
    "        if not heading_h2:\n",
    "            logger.warning(\"  âš  Section 'Common_ingredients' not found on the page\")\n",
    "            return pd.DataFrame(columns=[\"ingredient\", \"wikipedia_link\", \"description\", \"source\"])\n",
    "\n",
    "        # â”€â”€ Navigate to the grandparent containing the content â”€â”€\n",
    "        # heading_h2 is inside a <div class=\"mw-heading mw-heading2\">\n",
    "        # the grandparent of this div contains all <p> elements of the section\n",
    "        heading_div = heading_h2.parent   # <div class=\"mw-heading mw-heading2\">\n",
    "        container   = heading_div.parent  # container with all <p>\n",
    "\n",
    "        logger.info(f\"  Container found: <{container.name}>\")\n",
    "\n",
    "        # â”€â”€ Iterate over the children of the container after heading_div â”€â”€\n",
    "        found_heading = False\n",
    "        for child in container.children:\n",
    "            # Start collecting after the \"Common_ingredients\" heading\n",
    "            if child is heading_div:\n",
    "                found_heading = True\n",
    "                continue\n",
    "\n",
    "            if not found_heading:\n",
    "                continue\n",
    "\n",
    "            # Stop at the next <div class=\"mw-heading mw-heading2\"> (= next section)\n",
    "            if child.name == \"div\" and \"mw-heading2\" in child.get(\"class\", []):\n",
    "                logger.info(f\"  End of section detected: '{child.get_text(strip=True)[:40]}'\")\n",
    "                break\n",
    "\n",
    "            # Process only <p> (paragraphs of content)\n",
    "            if child.name == \"p\":\n",
    "                paragraph_text = child.get_text(strip=True)\n",
    "\n",
    "                # Extract all <a href=\"/wiki/...\"> links in this paragraph\n",
    "                for a in child.find_all(\"a\", href=True):\n",
    "                    href = a[\"href\"]\n",
    "                    # Keep only internal Wikipedia links (/wiki/...) and exclude special links\n",
    "                    if href.startswith(\"/wiki/\") and not href.startswith(\"/wiki/Special:\"):\n",
    "                        ingredient_name = a.get_text(strip=True)\n",
    "                        # Filter: ignore very short or generic texts\n",
    "                        if len(ingredient_name) > 2:\n",
    "                            scraped_ingredients.append({\n",
    "                                \"ingredient\":      ingredient_name.upper(),\n",
    "                                \"wikipedia_link\":  \"https://en.wikipedia.org\" + href,\n",
    "                                \"description\":     paragraph_text[:200],\n",
    "                                \"source\":          \"wikipedia_scraping\"\n",
    "                            })\n",
    "\n",
    "        # â”€â”€ Deduplicate by ingredient name â”€â”€\n",
    "        df = pd.DataFrame(scraped_ingredients)\n",
    "        if not df.empty:\n",
    "            df = df.drop_duplicates(subset=[\"ingredient\"]).reset_index(drop=True)\n",
    "\n",
    "        logger.info(f\"  âœ“ Scraping â€” total: {len(df)} ingredients extracted\")\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(\"  âœ— Timeout during Wikipedia scraping\")\n",
    "        return pd.DataFrame(columns=[\"ingredient\", \"wikipedia_link\", \"description\", \"source\"])\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.error(f\"  âœ— HTTP error during scraping: {e}\")\n",
    "        return pd.DataFrame(columns=[\"ingredient\", \"wikipedia_link\", \"description\", \"source\"])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"  âœ— Network error during scraping: {e}\")\n",
    "        return pd.DataFrame(columns=[\"ingredient\", \"wikipedia_link\", \"description\", \"source\"])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"  âœ— Unexpected error during scraping: {e}\")\n",
    "        return pd.DataFrame(columns=[\"ingredient\", \"wikipedia_link\", \"description\", \"source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b134c2-4f9c-4d11-87ec-5d61eada85c6",
   "metadata": {},
   "source": [
    "### 4. EXTRACTION FROM THE MySQL DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567aac99-b1eb-4850-8665-b59878c69f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_database() -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Connects to the MySQL database and extracts data from the\n",
    "    sephora_products and skincare_products tables.\n",
    "    Returns a dictionary {table_name: DataFrame}.\n",
    "    \"\"\"\n",
    "    logger.info(\"â”€â”€ Source 4: Extraction from the MySQL database â”€â”€\")\n",
    "\n",
    "    connection_string = (\n",
    "        f\"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}\"\n",
    "        f\"@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}\"\n",
    "    )\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"  Connecting to MySQL: {MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB} ...\")\n",
    "        engine = create_engine(connection_string, pool_pre_ping=True)\n",
    "\n",
    "        # â”€â”€ Test connection â”€â”€\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"SELECT 1\"))\n",
    "        logger.info(\"  âœ“ MySQL connection successful\")\n",
    "\n",
    "        # â”€â”€ Extract sephora_products table â”€â”€\n",
    "        try:\n",
    "            query_sephora = text(\"\"\"\n",
    "                SELECT\n",
    "                    product_id,\n",
    "                    product_name,\n",
    "                    brand_name,\n",
    "                    product_type,\n",
    "                    price_usd,\n",
    "                    rating,\n",
    "                    has_restricted_ingredient,\n",
    "                    has_cmr,\n",
    "                    restricted_ingredient_count,\n",
    "                    cmr_count\n",
    "                FROM sephora_products\n",
    "                WHERE product_name IS NOT NULL\n",
    "                ORDER BY product_id\n",
    "            \"\"\")\n",
    "            df_sephora = pd.read_sql(query_sephora, engine)\n",
    "            results[\"db_sephora\"] = df_sephora\n",
    "            logger.info(f\"  âœ“ sephora_products: {len(df_sephora)} rows extracted\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"  âœ— Error extracting sephora_products: {e}\")\n",
    "            results[\"db_sephora\"] = pd.DataFrame()\n",
    "\n",
    "        # â”€â”€ Extract skincare_products table â”€â”€\n",
    "        try:\n",
    "            query_skincare = text(\"\"\"\n",
    "                SELECT\n",
    "                    brand,\n",
    "                    product_name,\n",
    "                    product_type,\n",
    "                    price,\n",
    "                    rating,\n",
    "                    has_restricted_ingredient,\n",
    "                    has_cmr,\n",
    "                    restricted_ingredient_count,\n",
    "                    cmr_count\n",
    "                FROM skincare_products\n",
    "                WHERE product_name IS NOT NULL\n",
    "                ORDER BY brand, product_name\n",
    "            \"\"\")\n",
    "            df_skincare = pd.read_sql(query_skincare, engine)\n",
    "            results[\"db_skincare\"] = df_skincare\n",
    "            logger.info(f\"  âœ“ skincare_products: {len(df_skincare)} rows extracted\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"  âœ— Error extracting skincare_products: {e}\")\n",
    "            results[\"db_skincare\"] = pd.DataFrame()\n",
    "\n",
    "        engine.dispose()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"  âœ— Unable to connect to MySQL: {e}\")\n",
    "        logger.info(\"  âš  MySQL source ignored â€” script continues with other sources.\")\n",
    "        results[\"db_sephora\"]  = pd.DataFrame()\n",
    "        results[\"db_skincare\"] = pd.DataFrame()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf3c0e-3cba-4c59-aa5e-fc2e3dbf6fb2",
   "metadata": {},
   "source": [
    "### 5. SAVE RAW EXTRACTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ff06375-0adc-45fe-8fc5-994f3a57f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw(name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Save a raw DataFrame into the data/raw folder with a timestamp.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(f\"  âš  DataFrame '{name}' is empty â€” nothing saved.\")\n",
    "        return\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = RAW_DIR / f\"{name}_{timestamp}.csv\"\n",
    "    df.to_csv(filepath, index=False, encoding=\"utf-8\")\n",
    "    logger.info(f\"  ðŸ’¾ Saved: {filepath} ({len(df)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35de3f4-4b7d-470a-9b0c-66a562ad67ca",
   "metadata": {},
   "source": [
    "### 6. MAIN ENTRY POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "602efd4d-7c7b-4fff-bca7-5c72de10f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-06 21:22:13 [INFO] ============================================================\n",
      "2026-02-06 21:22:13 [INFO]   START OF EXTRACTION â€” 2026-02-06 21:22:13\n",
      "2026-02-06 21:22:13 [INFO] ============================================================\n",
      "2026-02-06 21:22:13 [INFO] â”€â”€ Source 1: Extraction from local files â”€â”€\n",
      "2026-02-06 21:22:13 [INFO]   Reading data/raw/COSING_Annex_III_v2.xls ...\n",
      "2026-02-06 21:22:13 [INFO]   âœ“ COSING loaded: 375 rows, 16 columns\n",
      "2026-02-06 21:22:13 [INFO]   Reading data/raw/product_info.csv ...\n",
      "2026-02-06 21:22:13 [INFO]   âœ“ SEPHORA loaded: 8494 rows, 27 columns\n",
      "2026-02-06 21:22:13 [INFO]   Reading data/raw/cosmetics.csv ...\n",
      "2026-02-06 21:22:13 [INFO]   âœ“ SKINCARE loaded: 1472 rows, 11 columns\n",
      "2026-02-06 21:22:13 [INFO]   ðŸ’¾ Saved: C:\\Users\\ZINA\\Desktop\\IRONHACK\\Week_8\\cosmetics\\data\\raw\\cosing_20260206_212213.csv (375 rows)\n",
      "2026-02-06 21:22:14 [INFO]   ðŸ’¾ Saved: C:\\Users\\ZINA\\Desktop\\IRONHACK\\Week_8\\cosmetics\\data\\raw\\sephora_20260206_212213.csv (8494 rows)\n",
      "2026-02-06 21:22:14 [INFO]   ðŸ’¾ Saved: C:\\Users\\ZINA\\Desktop\\IRONHACK\\Week_8\\cosmetics\\data\\raw\\skincare_20260206_212214.csv (1472 rows)\n",
      "2026-02-06 21:22:14 [INFO] â”€â”€ Source 2: Extraction via REST API (Open Beauty Facts â€” JSON) â”€â”€\n",
      "2026-02-06 21:22:14 [INFO]   API request page 1/3 ...\n",
      "2026-02-06 21:22:28 [INFO]   âœ“ Page 1: 50 products retrieved\n",
      "2026-02-06 21:22:28 [INFO]   API request page 2/3 ...\n",
      "2026-02-06 21:22:33 [INFO]   âœ“ Page 2: 41 products retrieved\n",
      "2026-02-06 21:22:33 [INFO]   API request page 3/3 ...\n",
      "2026-02-06 21:22:35 [INFO]   No products on page 3, stopping pagination.\n",
      "2026-02-06 21:22:35 [INFO]   âœ“ REST API â€” total: 91 products extracted\n",
      "2026-02-06 21:22:35 [INFO]   ðŸ’¾ Saved: C:\\Users\\ZINA\\Desktop\\IRONHACK\\Week_8\\cosmetics\\data\\raw\\api_open_beauty_facts_20260206_212235.csv (91 rows)\n",
      "2026-02-06 21:22:35 [INFO] â”€â”€ Source 3: Extraction via Web Scraping (Wikipedia â€” Ingredients of cosmetics) â”€â”€\n",
      "2026-02-06 21:22:35 [INFO]   GET request to https://en.wikipedia.org/wiki/Ingredients_of_cosmetics ...\n",
      "2026-02-06 21:22:36 [INFO]   âœ“ HTML page retrieved â€” size: 153036 characters\n",
      "2026-02-06 21:22:36 [INFO]   Container found: <meta>\n",
      "2026-02-06 21:22:36 [INFO]   End of section detected: 'Types of cosmetics[edit]'\n",
      "2026-02-06 21:22:36 [INFO]   âœ“ Scraping â€” total: 20 ingredients extracted\n",
      "2026-02-06 21:22:36 [INFO]   ðŸ’¾ Saved: C:\\Users\\ZINA\\Desktop\\IRONHACK\\Week_8\\cosmetics\\data\\raw\\scraping_wikipedia_cosmetics_20260206_212236.csv (20 rows)\n",
      "2026-02-06 21:22:36 [INFO] â”€â”€ Source 4: Extraction from the MySQL database â”€â”€\n",
      "2026-02-06 21:22:36 [INFO]   Connecting to MySQL: localhost:3306/cosmetics_regulatory_db ...\n",
      "2026-02-06 21:22:36 [INFO]   âœ“ MySQL connection successful\n",
      "2026-02-06 21:22:37 [INFO]   âœ“ sephora_products: 7380 rows extracted\n",
      "2026-02-06 21:22:37 [INFO]   âœ“ skincare_products: 1352 rows extracted\n",
      "2026-02-06 21:22:37 [INFO]   ðŸ’¾ Saved: C:\\Users\\ZINA\\Desktop\\IRONHACK\\Week_8\\cosmetics\\data\\raw\\db_sephora_20260206_212237.csv (7380 rows)\n",
      "2026-02-06 21:22:37 [INFO]   ðŸ’¾ Saved: C:\\Users\\ZINA\\Desktop\\IRONHACK\\Week_8\\cosmetics\\data\\raw\\db_skincare_20260206_212237.csv (1352 rows)\n",
      "2026-02-06 21:22:37 [INFO] ============================================================\n",
      "2026-02-06 21:22:37 [INFO]   EXTRACTION REPORT\n",
      "2026-02-06 21:22:37 [INFO] ============================================================\n",
      "2026-02-06 21:22:37 [INFO]   âœ“  file_cosing............................. 375 rows\n",
      "2026-02-06 21:22:37 [INFO]   âœ“  file_sephora............................ 8494 rows\n",
      "2026-02-06 21:22:37 [INFO]   âœ“  file_skincare........................... 1472 rows\n",
      "2026-02-06 21:22:37 [INFO]   âœ“  api_open_beauty_facts................... 91 rows\n",
      "2026-02-06 21:22:37 [INFO]   âœ“  scraping_wikipedia...................... 20 rows\n",
      "2026-02-06 21:22:37 [INFO]   âœ“  db_sephora.............................. 7380 rows\n",
      "2026-02-06 21:22:37 [INFO]   âœ“  db_skincare............................. 1352 rows\n",
      "2026-02-06 21:22:37 [INFO]   Total duration: 24.48 seconds\n",
      "2026-02-06 21:22:37 [INFO] ============================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Entry point of the script.\n",
    "    Orchestrates extraction from the 4 sources,\n",
    "    saves the raw results, and produces a summary report.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"  START OF EXTRACTION â€” \" + start_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "    extraction_report = {}  # for the final report\n",
    "\n",
    "    # â”€â”€ 1. Local files â”€â”€\n",
    "    try:\n",
    "        files_data = extract_from_files()\n",
    "        for name, df in files_data.items():\n",
    "            save_raw(name, df)\n",
    "            extraction_report[f\"file_{name}\"] = len(df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unhandled error in extract_from_files: {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "    # â”€â”€ 2. REST API â”€â”€\n",
    "    try:\n",
    "        api_data = extract_from_api(max_pages=3)\n",
    "        save_raw(\"api_open_beauty_facts\", api_data)\n",
    "        extraction_report[\"api_open_beauty_facts\"] = len(api_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unhandled error in extract_from_api: {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "    # â”€â”€ 3. Web Scraping â”€â”€\n",
    "    try:\n",
    "        scraping_data = extract_by_scraping()\n",
    "        save_raw(\"scraping_wikipedia_cosmetics\", scraping_data)\n",
    "        extraction_report[\"scraping_wikipedia\"] = len(scraping_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unhandled error in extract_by_scraping: {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "    # â”€â”€ 4. MySQL Database â”€â”€\n",
    "    try:\n",
    "        db_data = extract_from_database()\n",
    "        for name, df in db_data.items():\n",
    "            save_raw(name, df)\n",
    "            extraction_report[name] = len(df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unhandled error in extract_from_database: {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "    # â”€â”€ Summary Report â”€â”€\n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"  EXTRACTION REPORT\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    for source, count in extraction_report.items():\n",
    "        status = \"âœ“\" if count > 0 else \"âœ— (empty)\"\n",
    "        logger.info(f\"  {status}  {source:.<40} {count} rows\")\n",
    "    logger.info(f\"  Total duration: {duration:.2f} seconds\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "\n",
    "# â”€â”€ Execution â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb17f0e-102d-48c7-a929-a0f15825ad95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cosmetics)",
   "language": "python",
   "name": "cosmetics-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
