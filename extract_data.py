"""
extract_data.py
===============
Script d'extraction de donnÃ©es pour le projet "Regulatory Analysis of Cosmetic Ingredients".

Sources d'extraction :
    1. Fichiers locaux       â†’ COSING_Annex_III_v2.xls, product_info.csv, cosmetics.csv
    2. API REST              â†’ Open Beauty Facts API (produits cosmÃ©tiques, format JSON)
    3. Web Scraping          â†’ Page HTML Open Beauty Facts (mÃªme site, mais parsing HTML avec BeautifulSoup)
    4. Base de donnÃ©es MySQL â†’ Tables sephora_products / skincare_products dÃ©jÃ  importÃ©es

ExÃ©cution :
    python extract_data.py
    OU dans un notebook Jupyter : exÃ©cuter les cellules de haut en bas

DÃ©pendances (voir requirements.txt) :
    pandas, openpyxl, xlrd, requests, beautifulsoup4, lxml, pymysql, sqlalchemy, python-dotenv
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. POINT DE LANCEMENT & INITIALISATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import sys
import os
import logging
import traceback
from datetime import datetime
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text
from dotenv import load_dotenv

# â”€â”€ Charger les variables d'environnement (.env) â”€â”€
load_dotenv()

# â”€â”€ Configuration des chemins â”€â”€
# Path.cwd() marche dans un notebook Jupyter ET en ligne de commande
BASE_DIR      = Path.cwd()
RAW_DIR       = BASE_DIR / "data" / "raw"
PROCESSED_DIR = BASE_DIR / "data" / "processed"
RAW_DIR.mkdir(parents=True, exist_ok=True)
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

# â”€â”€ Configuration du logging â”€â”€
LOG_FILE = BASE_DIR / "logs" / "extraction.log"
LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# â”€â”€ Lecture des paramÃ¨tres depuis .env â”€â”€
MYSQL_USER     = os.getenv("MYSQL_USER",     "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "")
MYSQL_HOST     = os.getenv("MYSQL_HOST",     "localhost")
MYSQL_PORT     = os.getenv("MYSQL_PORT",     "3306")
MYSQL_DB       = os.getenv("MYSQL_DB",       "cosmetics_regulatory_db")

# Chemins des fichiers locaux
FILE_COSING   = os.getenv("FILE_COSING",   str(BASE_DIR / "data" / "raw" / "COSING_Annex_III_v2.xls"))
FILE_SEPHORA  = os.getenv("FILE_SEPHORA",  str(BASE_DIR / "data" / "raw" / "product_info.csv"))
FILE_SKINCARE = os.getenv("FILE_SKINCARE", str(BASE_DIR / "data" / "raw" / "cosmetics.csv"))

# URL pour l'API REST et le scraping
API_OPEN_BEAUTY_FACTS_URL      = "https://world.openfoodfacts.org/cgi/search.pl"
SCRAPING_WIKIPEDIA_COSM_URL    = "https://en.wikipedia.org/wiki/Ingredients_of_cosmetics"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. EXTRACTION DEPUIS LES FICHIERS LOCAUX
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_from_files() -> dict[str, pd.DataFrame]:
    """
    Lit les trois fichiers de donnÃ©es brutes locaux.
    Retourne un dictionnaire {nom_source: DataFrame}.
    """
    logger.info("â”€â”€ Source 1 : Extraction depuis les fichiers locaux â”€â”€")
    results = {}

    # 1a. COSING (fichier Excel)
    try:
        logger.info(f"  Lecture de {FILE_COSING} ...")
        df = pd.read_excel(FILE_COSING, engine="xlrd")
        results["cosing"] = df
        logger.info(f"  âœ“ COSING chargÃ© : {df.shape[0]} lignes, {df.shape[1]} colonnes")
    except FileNotFoundError:
        logger.error(f"  âœ— Fichier introuvable : {FILE_COSING}")
        results["cosing"] = pd.DataFrame()
    except Exception as e:
        logger.error(f"  âœ— Erreur lors de la lecture COSING : {e}")
        results["cosing"] = pd.DataFrame()

    # 1b. SEPHORA (fichier CSV)
    try:
        logger.info(f"  Lecture de {FILE_SEPHORA} ...")
        df = pd.read_csv(FILE_SEPHORA)
        results["sephora"] = df
        logger.info(f"  âœ“ SEPHORA chargÃ© : {df.shape[0]} lignes, {df.shape[1]} colonnes")
    except FileNotFoundError:
        logger.error(f"  âœ— Fichier introuvable : {FILE_SEPHORA}")
        results["sephora"] = pd.DataFrame()
    except Exception as e:
        logger.error(f"  âœ— Erreur lors de la lecture SEPHORA : {e}")
        results["sephora"] = pd.DataFrame()

    # 1c. SKINCARE (fichier CSV)
    try:
        logger.info(f"  Lecture de {FILE_SKINCARE} ...")
        df = pd.read_csv(FILE_SKINCARE)
        results["skincare"] = df
        logger.info(f"  âœ“ SKINCARE chargÃ© : {df.shape[0]} lignes, {df.shape[1]} colonnes")
    except FileNotFoundError:
        logger.error(f"  âœ— Fichier introuvable : {FILE_SKINCARE}")
        results["skincare"] = pd.DataFrame()
    except Exception as e:
        logger.error(f"  âœ— Erreur lors de la lecture SKINCARE : {e}")
        results["skincare"] = pd.DataFrame()

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. EXTRACTION VIA API REST (Open Beauty Facts â€” JSON)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_from_api(max_pages: int = 3) -> pd.DataFrame:
    """
    Appelle l'API Open Beauty Facts avec json=1 â†’ on rÃ©cupÃ¨re du JSON directement.
    ParamÃ¨tre :
        max_pages : nombre de pages de rÃ©sultats Ã  rÃ©cupÃ©rer.
    Retourne un DataFrame avec les produits extraits.
    """
    logger.info("â”€â”€ Source 2 : Extraction via API REST (Open Beauty Facts â€” JSON) â”€â”€")

    all_products = []
    headers = {"User-Agent": "Cosmetics-Regulatory-Project/1.0 (educational)"}

    for page in range(1, max_pages + 1):
        try:
            logger.info(f"  RequÃªte API page {page}/{max_pages} ...")
            params = {
                "search_terms": "cosmetics",
                "search_simple": "1",
                "action": "process",
                "json": "1",            # â† on demande du JSON
                "page": page,
                "count": 25
            }
            response = requests.get(
                API_OPEN_BEAUTY_FACTS_URL,
                params=params,
                headers=headers,
                timeout=30
            )
            response.raise_for_status()

            data = response.json()
            products = data.get("products", [])

            if not products:
                logger.info(f"  Aucun produit sur la page {page}, arrÃªt de la pagination.")
                break

            for product in products:
                all_products.append({
                    "product_name":     product.get("product_name", ""),
                    "brand":            product.get("brands", ""),
                    "ingredients_text": product.get("ingredients_text", ""),
                    "categories":       product.get("categories", ""),
                    "country":          product.get("countries", ""),
                    "barcode":          product.get("code", ""),
                    "source":           "open_beauty_facts_api"
                })

            logger.info(f"  âœ“ Page {page} : {len(products)} produits rÃ©cupÃ©rÃ©s")

        except requests.exceptions.Timeout:
            logger.warning(f"  âš  Timeout sur la page {page} â€” on continue avec la suivante.")
            continue
        except requests.exceptions.HTTPError as e:
            logger.error(f"  âœ— Erreur HTTP page {page} : {e}")
            continue
        except requests.exceptions.RequestException as e:
            logger.error(f"  âœ— Erreur rÃ©seau page {page} : {e}")
            break
        except (ValueError, KeyError) as e:
            logger.error(f"  âœ— Erreur de parsing JSON page {page} : {e}")
            continue

    df = pd.DataFrame(all_products)
    logger.info(f"  âœ“ API REST â€” total : {len(df)} produits extraits")
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. EXTRACTION PAR WEB SCRAPING (Wikipedia â€” Ingredients of cosmetics)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_by_scraping() -> pd.DataFrame:
    """
    Scrape la page Wikipedia "Ingredients of cosmetics" pour extraire
    la liste des ingrÃ©dients cosmÃ©tiques courants et leurs liens.

    Structure du DOM confirmÃ©e :
        - Le heading est <h2 id="Common_ingredients"> Ã  l'intÃ©rieur d'un
          <div class="mw-heading mw-heading2">
        - Le grand-parent de ce <div> contient tous les <p> du contenu
        - Chaque <p> contient des liens <a href="/wiki/..."> vers les ingrÃ©dients
        - On s'arrÃªte quand on rencontre le prochain <div class="mw-heading mw-heading2">
          (qui est "Types of cosmetics")

    Retourne un DataFrame avec : ingredient, wikipedia_link, description, source
    """
    logger.info("â”€â”€ Source 3 : Extraction par Web Scraping (Wikipedia â€” Ingredients of cosmetics) â”€â”€")

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
    }

    scraped_ingredients = []

    try:
        logger.info(f"  RequÃªte GET vers {SCRAPING_WIKIPEDIA_COSM_URL} ...")
        response = requests.get(SCRAPING_WIKIPEDIA_COSM_URL, headers=headers, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        logger.info(f"  âœ“ Page HTML rÃ©cupÃ©rÃ©e â€” taille : {len(response.text)} caractÃ¨res")

        # â”€â”€ Trouver le heading "Common_ingredients" â”€â”€
        heading_h2 = soup.find("h2", {"id": "Common_ingredients"})
        if not heading_h2:
            logger.warning("  âš  Section 'Common_ingredients' introuvable sur la page")
            return pd.DataFrame(columns=["ingredient", "wikipedia_link", "description", "source"])

        # â”€â”€ Remonter au grand-parent qui contient le contenu â”€â”€
        # heading_h2 est Ã  l'intÃ©rieur d'un <div class="mw-heading mw-heading2">
        # le grand-parent de ce div contient tous les <p> de la section
        heading_div = heading_h2.parent   # <div class="mw-heading mw-heading2">
        container   = heading_div.parent  # le conteneur avec tous les <p>

        logger.info(f"  Container trouvÃ© : <{container.name}>")

        # â”€â”€ Parcourir les enfants du container aprÃ¨s le heading_div â”€â”€
        found_heading = False
        for child in container.children:
            # On commence Ã  collecter aprÃ¨s le heading "Common_ingredients"
            if child is heading_div:
                found_heading = True
                continue

            if not found_heading:
                continue

            # On s'arrÃªte au prochain <div class="mw-heading mw-heading2"> (= section suivante)
            if child.name == "div" and "mw-heading2" in child.get("class", []):
                logger.info(f"  Fin de section dÃ©tectÃ©e : '{child.get_text(strip=True)[:40]}'")
                break

            # On traite uniquement les <p> (paragraphes du contenu)
            if child.name == "p":
                paragraph_text = child.get_text(strip=True)

                # Extraire tous les liens <a href="/wiki/..."> dans ce paragraphe
                for a in child.find_all("a", href=True):
                    href = a["href"]
                    # On ne garde que les liens internes Wikipedia (/wiki/...)
                    # et on exclut les liens vers des references ([1], [2], etc.)
                    if href.startswith("/wiki/") and not href.startswith("/wiki/Special:"):
                        ingredient_name = a.get_text(strip=True)
                        # Filtrer : on ignore les textes trÃ¨s courts ou trÃ¨s gÃ©nÃ©riques
                        if len(ingredient_name) > 2:
                            scraped_ingredients.append({
                                "ingredient":      ingredient_name.upper(),
                                "wikipedia_link":  "https://en.wikipedia.org" + href,
                                "description":     paragraph_text[:200],
                                "source":          "wikipedia_scraping"
                            })

        # â”€â”€ DÃ©dupliquer par nom d'ingrÃ©dient â”€â”€
        df = pd.DataFrame(scraped_ingredients)
        if not df.empty:
            df = df.drop_duplicates(subset=["ingredient"]).reset_index(drop=True)

        logger.info(f"  âœ“ Scraping â€” total : {len(df)} ingrÃ©dients extraits")
        return df

    except requests.exceptions.Timeout:
        logger.error("  âœ— Timeout lors du scraping Wikipedia")
        return pd.DataFrame(columns=["ingredient", "wikipedia_link", "description", "source"])
    except requests.exceptions.HTTPError as e:
        logger.error(f"  âœ— Erreur HTTP lors du scraping : {e}")
        return pd.DataFrame(columns=["ingredient", "wikipedia_link", "description", "source"])
    except requests.exceptions.RequestException as e:
        logger.error(f"  âœ— Erreur rÃ©seau lors du scraping : {e}")
        return pd.DataFrame(columns=["ingredient", "wikipedia_link", "description", "source"])
    except Exception as e:
        logger.error(f"  âœ— Erreur inattendue lors du scraping : {e}")
        return pd.DataFrame(columns=["ingredient", "wikipedia_link", "description", "source"])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. EXTRACTION DEPUIS LA BASE DE DONNÃ‰ES MySQL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_from_database() -> dict[str, pd.DataFrame]:
    """
    Se connecte Ã  la base MySQL et extrait les donnÃ©es des tables
    sephora_products et skincare_products.
    Retourne un dictionnaire {table_name: DataFrame}.
    """
    logger.info("â”€â”€ Source 4 : Extraction depuis la base de donnÃ©es MySQL â”€â”€")

    connection_string = (
        f"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}"
        f"@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}"
    )
    results = {}

    try:
        logger.info(f"  Connexion Ã  MySQL : {MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB} ...")
        engine = create_engine(connection_string, pool_pre_ping=True)

        # â”€â”€ Test de connexion â”€â”€
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        logger.info("  âœ“ Connexion MySQL rÃ©ussie")

        # â”€â”€ Extraction table sephora_products â”€â”€
        try:
            query_sephora = text("""
                SELECT
                    product_id,
                    product_name,
                    brand_name,
                    product_type,
                    price_usd,
                    rating,
                    has_restricted_ingredient,
                    has_cmr,
                    restricted_ingredient_count,
                    cmr_count
                FROM sephora_products
                WHERE product_name IS NOT NULL
                ORDER BY product_id
            """)
            df_sephora = pd.read_sql(query_sephora, engine)
            results["db_sephora"] = df_sephora
            logger.info(f"  âœ“ sephora_products : {len(df_sephora)} lignes extraites")
        except Exception as e:
            logger.error(f"  âœ— Erreur extraction sephora_products : {e}")
            results["db_sephora"] = pd.DataFrame()

        # â”€â”€ Extraction table skincare_products â”€â”€
        try:
            query_skincare = text("""
                SELECT
                    brand,
                    product_name,
                    product_type,
                    price,
                    rating,
                    has_restricted_ingredient,
                    has_cmr,
                    restricted_ingredient_count,
                    cmr_count
                FROM skincare_products
                WHERE product_name IS NOT NULL
                ORDER BY brand, product_name
            """)
            df_skincare = pd.read_sql(query_skincare, engine)
            results["db_skincare"] = df_skincare
            logger.info(f"  âœ“ skincare_products : {len(df_skincare)} lignes extraites")
        except Exception as e:
            logger.error(f"  âœ— Erreur extraction skincare_products : {e}")
            results["db_skincare"] = pd.DataFrame()

        engine.dispose()

    except Exception as e:
        logger.error(f"  âœ— Impossible de se connecter Ã  MySQL : {e}")
        logger.info("  âš  Source MySQL ignorÃ©e â€” le script continue avec les autres sources.")
        results["db_sephora"]  = pd.DataFrame()
        results["db_skincare"] = pd.DataFrame()

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. SAUVEGARDE DES DONNÃ‰ES BRUTES EXTRAITES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_raw(name: str, df: pd.DataFrame) -> None:
    """Sauvegarde un DataFrame brut dans le dossier data/raw avec un horodatage."""
    if df.empty:
        logger.warning(f"  âš  DataFrame '{name}' vide â€” pas de sauvegarde.")
        return
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filepath = RAW_DIR / f"{name}_{timestamp}.csv"
    df.to_csv(filepath, index=False, encoding="utf-8")
    logger.info(f"  ðŸ’¾ SauvegardÃ© : {filepath} ({len(df)} lignes)")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. POINT D'ENTRÃ‰E PRINCIPAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    """
    Point d'entrÃ©e du script.
    Orchestre l'extraction depuis les 4 sources,
    sauvegarde les rÃ©sultats bruts, puis produit un rapport de synthÃ¨se.
    """
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("  DÃ‰BUT DE L'EXTRACTION â€” " + start_time.strftime("%Y-%m-%d %H:%M:%S"))
    logger.info("=" * 60)

    extraction_report = {}  # pour le rapport final

    # â”€â”€ 1. Fichiers locaux â”€â”€
    try:
        files_data = extract_from_files()
        for name, df in files_data.items():
            save_raw(name, df)
            extraction_report[f"fichier_{name}"] = len(df)
    except Exception as e:
        logger.error(f"Erreur non gÃ©rÃ©e dans extract_from_files : {e}\n{traceback.format_exc()}")

    # â”€â”€ 2. API REST â”€â”€
    try:
        api_data = extract_from_api(max_pages=3)
        save_raw("api_open_beauty_facts", api_data)
        extraction_report["api_open_beauty_facts"] = len(api_data)
    except Exception as e:
        logger.error(f"Erreur non gÃ©rÃ©e dans extract_from_api : {e}\n{traceback.format_exc()}")

    # â”€â”€ 3. Web Scraping â”€â”€
    try:
        scraping_data = extract_by_scraping()
        save_raw("scraping_wikipedia_cosmetics", scraping_data)
        extraction_report["scraping_wikipedia"] = len(scraping_data)
    except Exception as e:
        logger.error(f"Erreur non gÃ©rÃ©e dans extract_by_scraping : {e}\n{traceback.format_exc()}")

    # â”€â”€ 4. Base de donnÃ©es MySQL â”€â”€
    try:
        db_data = extract_from_database()
        for name, df in db_data.items():
            save_raw(name, df)
            extraction_report[name] = len(df)
    except Exception as e:
        logger.error(f"Erreur non gÃ©rÃ©e dans extract_from_database : {e}\n{traceback.format_exc()}")

    # â”€â”€ Rapport de synthÃ¨se â”€â”€
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    logger.info("=" * 60)
    logger.info("  RAPPORT D'EXTRACTION")
    logger.info("=" * 60)
    for source, count in extraction_report.items():
        status = "âœ“" if count > 0 else "âœ— (vide)"
        logger.info(f"  {status}  {source:.<40} {count} lignes")
    logger.info(f"  DurÃ©e totale : {duration:.2f} secondes")
    logger.info("=" * 60)


# â”€â”€ ExÃ©cution â”€â”€
if __name__ == "__main__":
    main()
